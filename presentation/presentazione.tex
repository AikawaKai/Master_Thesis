\documentclass[9pt]{beamer}
\usetheme[titlepagelogo=minerva2,% Logo for the first page
						language=italian
                        ]{TorinoTh}
                        
\usepackage[beamer,customcolors]{hf-tikz}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\hfsetfillcolor{alerted text.fg!10}
\hfsetbordercolor{alerted text.fg}

\author{Marco Odore}
\rel{Prof. Giorgio Valentini}
\assistantsupervisor{Dr. Marco Notaro}
\title[Metodi di Ensemble Gerarchici]{Metodi di Ensemble Gerarchici per la Predizione Strutturata della Funzione delle Proteine}
\ateneo{Università Degli Studi Di Milano}
\date{10 Luglio 2018}

\begin{document}
\titlepageframe
\begin{tframe}{Il problema della predizione della funzione delle proteine}
  % 
  \begin{columns}
    %
    \begin{column}{.65\textwidth}
      \minipage[c][0.4\textheight][s]{\columnwidth}
	   \begin{itemize}	
      \onslide<1->
	  \item Identificare la funzione delle proteine attraverso le analisi di laboratorio è \highlightbf{costosa} e richiede \highlightbf{molto tempo}
	  \onslide<2->
	  \item Esistono centinaia di funzioni a cui poter associare un gene/proteina, anche contemporaneamente \highlightbf{(problema multiclasse e multietichetta)}
      \onslide<3->
	  \item Il quantitativo di dati genomici cresce molto rapidamente.
      \onslide<4->
	  \item La \highlightbf{classificazione manuale} delle proteine è quindi infattibile. È necessario quindi un approccio \highlightbf{automatico}.
	  \onslide<5->
	  \item A complicare ulteriormente il problema è il modo in cui sono \emph{relazionate} tra loro le funzioni delle proteine.
	  \end{itemize}
      \endminipage      
    \end{column}
    %
    \begin{column}{.35\textwidth}
      % for top aligned images use minipage
      \only<1-5>{
        \minipage[c][0.4\textheight][s]{\columnwidth}
        \onslide<1->    
        \only<1-5>{
          \begin{figure}
            \centering
            \includegraphics<1>[scale=0.15]{img/lab3.jpg} %         
            \includegraphics<2>[scale=0.3]{img/multilabel.png}%
            \includegraphics<3>[scale=0.16]{img/growth.jpg}
            \includegraphics<4>[scale=0.1]{img/machinelearning.png}
            \includegraphics<5>[scale=0.3]{img/relation.png}
        \end{figure}}
       \endminipage
      }   
      % for vertically centered images use parbox
    \end{column}
  \end{columns}
\end{tframe}

\begin{tframe}{Tassonomie per le funzioni delle proteine}
\begin{itemize}
\onslide<1->
\item Esistono infatti due tassonomie principali per l'organizzazione delle funzioni:
\begin{itemize}

\onslide<2->
\item \highlightbf{Gene Ontology} (GO):  che organizza le funzioni come un grafo diretto aciclico (DAG), varia per ogni specie, e possiede tre ontologie differenti (e quindi 3 DAG), e cioè \highlight{Biological Process} (BP), \highlight{Molecular Function} (MF) e \highlight{Cellular Component} (CC). 
\onslide<3->
\item \highlightbf{Functional Catalogue} (FunCat): che è organizzato invece come un albero, non varia in base alle specie, e descrive le funzioni in maniera più sintetica rispetto alla Gene Ontology.
\end{itemize}
\onslide<4->
\begin{figure}[h]
\center
\includegraphics[scale=0.15]{./img/GO.png}
\includegraphics[scale=0.14]{./img/FunCat.png}
\label{DAGTREE}
\end{figure}
\onslide<5->
\item Data la granularità e specificità superiori della GO e il suo largo utilizzo nella comunità scientifica, all’interno della tesi ci si è soffermati sulla predizione delle sue funzioni.
\end{itemize}  
\end{tframe}

\begin{tframe}{La predizione della funzione delle proteine tramite metodi automatici}
I metodi più noti in letteratura per effettuare predizioni della funzione delle proteine in maniera automatica sono:

\begin{itemize}
\onslide<2->
\item I metodi basati sulla \highlightbf{comparazione di biosequenze}: si basano sull'idea che sequenze simili condividano funzioni simili.
\onslide<3->
\item I metodi \highlightbf{basati su reti}: sono metodi applicati a dati rappresentati sotto forma di reti, che si basano sugli algoritmi di propagazione delle etichette.
\onslide<4->
\item I metodi \highlightbf{Kernel per spazi di output strutturato}: sono metodi che sfruttano funzioni kernel congiunte per predire in spazi di output strutturato.
\onslide<5->
\item I metodi \highlightbf{Ensemble Gerarchici}: i metodi trattati in questa tesi.
\end{itemize}

\end{tframe}

\begin{tframe}{Metodi Ensemble Gerarchici 1/2}
I Metodi di Ensemble Gerarchici sono metodi caratterizzati da due step principali:

\begin{enumerate}
\onslide<2->
\item \highlightbf{Predizione flat} delle diverse classi dell’ontologia, generando diversi predittori \emph{indipendenti}.
\onslide<3->
\item \highlightbf{Combinazione e correzione gerarchica delle predizioni} sfruttando il DAG dei termini della GO.
\end{enumerate}
\onslide<4->
Il secondo step rappresenta la componente \emph{ensemble} del metodo. Tale step si rende necessario in quanto le predizioni flat non tengono in considerazione la struttura gerarchica dei DAG della GO, portando a risultati \emph{inconsistenti}.
\onslide<5->
\block{Consistenza \& True Path Rule}
Un insieme di predizioni $\hat{y} = <\hat{y}_1, \hat{y}_2, \dots, \hat{y}_{|N|}>$, dove $|N|$ è la cardinalità dei termini della gerarchia, è definito \emph{consistente}, se rispetta la \emph{True Path Rule}, e cioè:
\[
y\;\;\;consistente\;\; \leftrightarrow \forall i \in N, j \in par(i) \rightarrow y_j \geq y_i
\] 
Dove $par(i)$ indica l'insieme dei termini genitori del nodo $i$ nella gerarchia.
\endblock{}
\end{tframe}

\begin{tframe}{Metodi Ensemble Gerarchici (Esempio) 2/2}
\begin{center}
\includegraphics<1>[width=5cm]{img/1_1.png}
\includegraphics<2>[width=5cm]{img/2.png}
\includegraphics<3>[width=5cm]{img/3.png}
\includegraphics<4>[width=8.22cm]{img/4.png}
\end{center}

\end{tframe} 


\begin{tframe}{Metodi Ensemble Gerarchici: Approcci}
\begin{columns}
    \begin{column}{.50\textwidth}
      \minipage[c][0.4\textheight][s]{\columnwidth}
      Esistono fondamentalmente due approcci per la correzione:
	   \begin{itemize}
	   \onslide<2->
	  \item \highlight{Top-down}: le predizioni vengono corrette dai nodi più generali a quelli più specifici.
	   \onslide<3->	  
	  \item \highlight{Bottom-up}: Le predizioni vengono corrette dai nodi più specifici verso quelli più generali.
      \end{itemize}
      \endminipage 
    \end{column}
    %
    \begin{column}{.50\textwidth}
    \onslide<2-> 
        \minipage[c][0.4\textheight][s]{\columnwidth}
        \only<2-3>{
          \begin{figure}
            \centering
            \includegraphics<2>[scale=0.3]{img/topdown.png}
            \includegraphics<3>[scale=0.3]{img/bottomup.png}
        \end{figure}}
        \endminipage
    \end{column}
  \end{columns}
\end{tframe}

\begin{tframe}{Metodo Top-Down Gerarchico (HTD-DAG)}
\begin{itemize}

\onslide<1->
\item È un metodo che utilizza l'approccio \emph{Top-Down}.
\onslide<2->
\item La correzione avviene ricorsivamente, percorrendo il grafo per \emph{livelli}\footnote{\footnotesize{Dove il livello è quello del cammino massimo dalla radice}}. Più precisamente, dato il grafo $G = (N, E)$, gli score flat $f(x) = \hat{y}$ sono corretti gerarchicamente a $\bar{y}$, applicando la seguente regola:
\begin{block}{Aggiornamento con HTD-DAG}
\[
\bar{y}_i := 
\begin{cases}
\hat{y}_i \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; if\;\; i \in root(G)\\
min_{j \in par(i)} \bar{y}_j \;\;\;\; if \;\; min_{j \in par(i)}\bar{y}_j < \hat{y}_i\\
\hat{y}_i \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; altrimenti
\end{cases}
\]
\end{block}
Dove $par(i)$ specifica i genitori del nodo $i$.
\end{itemize} 
\end{tframe}

\begin{tframe}{Metodo True Path Rule per DAG (TPR-DAG)}
\begin{itemize}
\onslide<1->
\item È un metodo che combina gli approcci top-down e bottom-up per la correzione delle predizioni flat. 
\onslide<2->
\item È suddiviso in due step sequenziali:
\begin{enumerate}
\onslide<3->
\item \highlightbf{Step bottom-up}: che partendo dai nodi più specifici del DAG, propaga quelle predizioni flat che sono considerate \highlight{positive}.
\onslide<4->
\item \highlightbf{Step top-down}: È il medesimo step utilizzato dal metodo HTD-DAG.
\end{enumerate}
\onslide<5->
\item Lo step top down si rende necessario in quanto la propagazione delle predizioni positive dal basso verso l’alto non garantisce la consistenza delle predizioni necessarie alla True Path Rule. 
\onslide<6->
\item La selezione dei nodi considerati \emph{positivi} può avvenire in diverse maniere: con \highlight{soglia adattiva}, \highlight{senza soglia} e con \highlight{soglia fissa}.
\end{itemize}
\end{tframe}

\begin{tframe}{Generalized Pool Adjacent Violator (GPAV) (1/4)}
\begin{itemize}
\onslide<1->
\item Per il passo Top-Down dell’algoritmo TPR-DAG (e HTD) è stato sviluppato un nuovo metodo all'interno di questa tesi, e cioè \highlightbf{Generalized Pool Adjacent Violator} (GPAV), un algoritmo che permette di risolvere i problemi di \highlightbf{Isotonic Regression}, definiti come:
\begin{block}{Isotonic Regression (caso generale con ordinamento parziale)}
Dato un DAG, $G(N, E)$, con il set di nodi $N = \{1, 2, ..., n\}$, si deve trovare il vettore $x^{*}\in R^{n}$ tale che:
\[
min \sum_{i=1}^{n} w_i (x_i - a_i)^2
\]
\begin{center}
such that $x_i \le x_j$ $\forall (i,j) \in E $ 
\end{center}
\end{block} 
\onslide<2->
\item Con una complessità pari a $O(n^2)$.
\end{itemize}
\end{tframe}


\begin{tframe}{Generalized Pool Adjacent Violator (GPAV) (2/4)}
Esempio di Isotonic Regression con \emph{ordinamento totale}:
\begin{figure}
\centering
\includegraphics[scale=0.3]{img/monotonic.png}
\end{figure}
\end{tframe}
\begin{tframe}{Generalized Pool Adjacent Violator (GPAV) (3/4)}
Funzionamento Generale:
\begin{itemize}
\onslide<2->
\item Per far funzionare l'algoritmo è necessario effettuare un \highlight{ordinamento topologico} del grafo.
\onslide<3->
\item L’algoritmo genera uno split del set di nodi $N$ del DAG, in un insieme di \highlight{blocchi disgiunti definiti da $H$} (inizialmente $H = N$).
\onslide<4->
\item Seguendo l'ordinamento topologico del grafo, un blocco \highlight{assorbe} un suo blocco \emph{predecessore} se si verificano determinate condizioni.
\onslide<5->
\item I nodi presenti nel medesimo blocco $B_i$ \highlight{condividono il medesimo valore} $x_i$, e quindi a seguito dell'assorbimento sarà necessario un aggiornamento di tale valore.
\end{itemize}
\end{tframe}

\begin{tframe}{Generalized Pool Adjacent Violator (GPAV e ISO-TPR) (4/4)}
\onslide<1->
\begin{center}
\scalebox{0.70}{
    \begin{minipage}{0.9\linewidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Procedure{GPAV}{}
\State $ H = N $
\For{(each $i \in N$)}
\State $B_i = \{i\}$ 
\State $B_i^{-} = i^{-}$
\State $x_i = \hat{y}_i$
\State $W_i = w_i$
\EndFor
\For{$k = 1, 2, ..., n$}
\State \footnotesize{\emph{//finché esiste un predecessore di $B_{k}$ che viola la monotonicità}}
\While{$\{i \in B_k^{-}: x_i > x_k\}\neq 0$} 
\State \footnotesize{\emph{// Trova l'elemento che viola maggiormente il vincolo}}
\State \textbf{Find} $j \in B_k^{-}: x_j = max\{x_i : i \in B_k^{-}\}$ 
\State \textbf{Absorb(k, j)} \emph{// $j$ viene assorbito da $B_k$}
\EndWhile
\EndFor
\EndProcedure
\end{algorithmic}
\caption{GPAV}
\end{algorithm}
\end{minipage}}
\end{center}
\begin{itemize}
\onslide<2->
\item Riassumendo, l’algoritmo effettua degli assorbimenti di blocchi adiacenti, finché questi violano i vincoli del problema quadratico, generando di fatto una partizione dei nodi, in cui le parti condividono lo stesso valore.
\onslide<3->
\item Sostituendo GPAV allo step Top-Down dell'algoritmo TPR-DAG visto in precedenza (invece che HTD), si ottiene l'algoritmo \highlightbf{ISO-TPR}, un altro nuovo metodo utilizzato in questa tesi.
\end{itemize}
\end{tframe}

\begin{tframe}{Predizione della funzione delle proteine in C.elegans (WORM)}
\begin{itemize}
\onslide<1->
\item Si è eseguita la sperimentazione sul genoma della specie
\highlight{Caenorhabditis elegans} (WORM), utilizzando come insieme delle  istanze e input del problema una matrice simmetrica generata dal network di interazione proteina-proteina \highlight{STRING}\footnote{\footnotesize{Search Tool for the Retrieval of Interacting Genes/Proteins}}.
\onslide<2->
\item Tale matrice STRING ha dimensione 15752×15752 (WORM). Il nostro problema ha quindi 15752 istanze.
\onslide<3->
\item In base al tipo do ontologia, si hanno DAG con un quantitativo diverso di nodi(termini) e archi(relazioni): 

\begin{table}[h]
\centering
\resizebox{.5\textwidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
      \textbf{ ontologia} & \textbf{numero di termini} & \textbf{numero di archi} \\ \hline
BP & 4068  &  8066   \\ 
\hline
MF  & 1163  & 1567   \\ 
\hline
CC  & 578  & 1082     \\ 
\hline
\end{tabular}}

\label{DAG_desc}
\end{table}
\end{itemize}
\end{tframe}

\begin{tframe}{Annotazioni per il dataset C.elegans (WORM)}
Per evitare di avere problemi nella fase di cross-validazione, i DAG sono stati ridotti a quei termini per cui si hanno almeno 10 annotazioni. Un po' di statistiche a seguito della selezione:
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Onto} & \textbf{numero di termini} & \textbf{media} & \textbf{d.std.} & \textbf{massimo} & \textbf{minimo} \\ \hline
BP            & 1335                & 71,33                & 151,68              & 2597                & 10                  \\ \hline
MF            & 186                 & 61,23                & 191,84              & 1806                & 10                  \\ \hline
CC            & 221                 & 131,9                & 302,25              & 1924                & 10                  \\ \hline
\end{tabular}
\caption{\footnotesize{La colonna \emph{numero di termini} indica il numero di termini ottenuti dopo la selezione, \emph{media} la media delle annotazioni per classe, per l'ontologia di riferimento, \emph{d.std.} la deviazione standard delle annotazioni per l'ontologia di riferimento, \emph{massimo} e \emph{minimo} rispettivamente il massimo e minimo numero di annotazioni.}}
\label{riduzioneann}
\end{table}
\end{tframe}

\begin{tframe}{Algoritmi di ML per le predizioni flat}
\begin{itemize}
\onslide<1->
\item Per quanto riguarda gli algoritmi di apprendmento automatico da utilizzare per le predizioni flat, si sono considerati i seguenti metodi:
\begin{enumerate}
\footnotesize{
\item \emph{K-Nearest Neighbors}
\item \emph{Logit Boost}
\item \emph{Linear Discriminant Analysis}
\item \emph{eXtreme Gradient Boosting} 
\item \emph{C5.0} (Alberi di decisione)
\item \emph{Random Forest}
\item \emph{Multilayer Perceptron}
\item \emph{Support Vector Machine lineare}
\item \emph{Bagged CART} (Bagged ensemble di alberi di decisione)
\item \emph{AdaBoost.M1}
\item \emph{Naive Bayes}
\item \emph{Glmnet}}
\end{enumerate}
\onslide<2->
\item Dato l'elevato numero di algoritmi selezionati per la sperimentazione, si è deciso di non effettuare il tuning dei parametri, questo per evitare di allungare ulteriorimente i tempi dell'intero processo di valutazione e generazione degli score flat.
\end{itemize}
\end{tframe}

\begin{tframe}{Cross-validazione e metriche usate per la valutazione degli algoritmi}
\begin{columns}

    \begin{column}{.65\textwidth}
      \minipage[c][0.4\textheight][s]{\columnwidth}
	   \begin{itemize}	
      		\onslide<1->
	  		\item Per stimare le performance dei nostri predittori si è utilizzata la tecnica della \highlight{cross-validazione}, la quale permette di valutare l’errore di un algoritmo di apprendimento stimando l’errore medio dei predittori prodotti  dall’algoritmo. 
	  		\onslide<2->
	  		\item Come metriche si sono poi usate la:
	  		\begin{enumerate}
	  			\onslide<3->
				\item \highlight{AUROC}: una misura di robustezza del classificatore,  che mette in relazione le misure di Recall e False Positive Rate, al variare di una soglia applicata all’output del modello.
				\onslide<4->	  			
				\item \highlight{AUPRC}: è una misura che mette in relazione la variazione di Precision al variare della Recall ed è utile nel caso di problemi sbilanciati. 
				\onslide<5->
	  			\item \highlight{F-Score gerarchica}: è una metrica centrata sui geni, massimizzata al variare di una soglia t in (0, 1). Tale misura si basa sulla Precisione e Recall centrate sui geni e non su le classi. Tale metrica è stata usata per il fit degli iperparametri dei metodi ensemble (soglia e pesi).
	  		\end{enumerate}
	  \end{itemize}
      \endminipage      
    \end{column}
    
    
    \begin{column}{.35\textwidth}
      % for top aligned images use minipage
      \only<1-5>{
        \minipage[c][0.4\textheight][s]{\columnwidth}
        \onslide<1->    
        \only<1-5>{
          \begin{figure}
            \centering
            \includegraphics<1>[scale=0.16]{img/cross10fold.png} %         
            \includegraphics<3>[scale=0.14]{img/AUROC.png}%
            \includegraphics<4>[scale=0.34]{img/AUPRC.png}
            \includegraphics<5>[scale=0.27]{img/Fscore.png}
        \end{figure}}
       \endminipage
      }   
      % for vertically centered images use parbox
    \end{column}


\end{columns} 
\end{tframe}

\begin{tframe}{Stima preliminare dei tempi di calcolo degli algoritmi flat}

\begin{center}
Cross-validation 10 fold - No tuning - Campione 10 classi
\end{center}
\begin{figure}[hp!]
\center
\includegraphics[scale=0.25]{../images/BP_box_plot_times.png}

\caption{\footnotesize{Il box plot dei tempi di esecuzione, con cross-validation a 10 fold per l' ontologia BP. I tempi sono da intendersi in ore e per classe, per un campione di 10 classi.}}
\label{boxplot_p}
\end{figure}
\end{tframe}

\begin{tframe}{Riduzione della complessità del problema}

\begin{columns}
    \begin{column}{.60\textwidth}
      \minipage[c][0.4\textheight][s]{\columnwidth}
		\begin{itemize}
			\onslide<1->
				\item Si è prima di tutto ridotto il numero di fold da 10 a 5 nella cross-validation
			\onslide<2->
				\item Si sono provati due metodi di riduzione della dimensionalità:
				\begin{enumerate}
				\onslide<3->
					\item \highlightbf{Selezione delle feature con Correlazione di Pearson} (FS).
				\onslide<4->
					\item \highlightbf{Selezione delle componenti con la Principal Component Analysis} (PCA)
				\end{enumerate}
		\onslide<5->
		\item Sempre facendo una stima sul medesimo campione di 10 classi, si sono testate:
		\begin{enumerate}
		\item configurazioni per la FS per le prime 1000, 500 e 100 feature nel ranking ottenuto (a destra BoxPlot per le prime 100 feature selezionate) 
		\onslide<6->
		\item configurazioni per la PCA, per le componenti che selezionano rispettivamente il 90\%, il 70\% e il 50\% della varianza spiegata (a destra BoxPlot per le prime 15 componenti = 50\% varianza spiegata)
		\end{enumerate}
		\end{itemize}	
	 \endminipage
	\end{column}
	
	\begin{column}{.40\textwidth}
      % for top aligned images use minipage
      \only<1-6>{
        \minipage[c][0.4\textheight][s]{\columnwidth}
        \onslide<1->    
        \only<1-6>{
          \begin{figure}
            \centering
            \includegraphics<1>[scale=0.26]{img/cross_val.png} %         
            \includegraphics<3>[scale=0.3]{img/Pearson.png}%
            \includegraphics<4>[scale=0.18]{img/PCA.png}
            \includegraphics<5>[scale=0.12]{../images/FS_BP_times.png}
            \includegraphics<5>[scale=0.12]{../images/FS_BP_auroc.png}
            \includegraphics<5>[scale=0.12]{../images/FS_BP_auprc.png}
            \includegraphics<6>[scale=0.12]{../images/PCA_BP_times.png}
            \includegraphics<6>[scale=0.12]{../images/PCA_BP_auroc.png}
            \includegraphics<6>[scale=0.12]{../images/PCA_BP_auprc.png}
        \end{figure}}
       \endminipage
      }  
      \end{column} 
\end{columns}
\end{tframe}

\begin{tframe}{Organizzazione degli esperimenti}
\onslide<1->
Data l'analisi precedente effettuata sugli algoritmi di apprendimento flat (su di un campione di 10 classi), si è deciso per queste due tipologie di configurazioni: 

\begin{itemize}
\item \highlightbf{cross-validation 5 fold + Selezione delle prime 100 feature con Correlazione di Pearson}.
\item \highlightbf{cross-validation 5 fold + Selezione delle prime 15 componenti della PCA}.
\end{itemize} 
Le quali hanno evidenziato un buon compromesso tra tempi di esecuzione e performance.  
\newline
\newline
\onslide<2->
Per quanto riguarda i metodi ensemble gerarchici, si sono utilizzati i seguenti metodi:

\begin{enumerate}
\item \highlightbf{HTD-DAG} 
\item \highlightbf{GPAV}
\item \highlightbf{TPR-DAG} (con variante senza soglia, con soglia adattiva, con pesi nell'aggiornamento)
\item \highlightbf{ISO-TPR} (con variante senza soglia, con soglia adattiva)
\end{enumerate}

\end{tframe}

\begin{tframe}{Performance degli algoritmi flat con FS e PCA}
La generazione di tutti gli score flat, per entrambi i metodi di riduzione della complessità, ha evidenziato delle performance generalmente migliori per la selezione delle feature effettuta con la correlazione di Pearson (Test dei ranghi con segno di Wilcoxon, con significatività $\alpha = 0.01$). Le performance migliori sono probabilmente da ricondurre al fatto che il tipo di riduzione per la correlazione è \emph{supervisionato} a differenza della PCA.

\begin{figure}[h!]
\centering
\hspace*{-0.3in}
\includegraphics<2>[scale=0.28]{../images/FS_vs_PCA_BP_AUROC.png}
\includegraphics<3>[scale=0.28]{../images/FS_vs_PCA_BP_AUPRC.png}
\end{figure}

\end{tframe}
\end{document}
